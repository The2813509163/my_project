08/22 15:19:52 - OpenCompass - INFO - Task [Llama-3.2-3B_hf/gsm8k_6,Llama-3.2-3B_hf/math_6,Llama-3.2-3B_hf/svamp_6,Llama-3.2-3B_hf/piqa_6,Llama-3.2-3B_hf/siqa_6,Llama-3.2-3B_hf/squad2.0_6,Llama-3.2-3B_hf/ARC-c_6,Llama-3.2-3B_hf/ARC-e_6,Llama-3.2-3B_hf/lambada_6]
08/22 15:20:00 - OpenCompass - WARNING - pad_token_id is not set for the tokenizer.
08/22 15:20:00 - OpenCompass - WARNING - Using eos_token_id 128001 as pad_token_id.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /data/kris/shared_data/models/Llama-3.2-3B and are newly initialized: ['model.HIO_A_Emb', 'model.HIO_B_Emb', 'model.HSM_mask_proxy', 'model.layers.0.HIO_A_Attn', 'model.layers.0.HIO_A_MLP', 'model.layers.0.HIO_B_Attn', 'model.layers.0.HIO_B_MLP', 'model.layers.0.mlp.HIO_A_down_proj', 'model.layers.0.mlp.HIO_B_down_proj', 'model.layers.0.mlp.HSM_mask_proxy', 'model.layers.1.HIO_A_Attn', 'model.layers.1.HIO_A_MLP', 'model.layers.1.HIO_B_Attn', 'model.layers.1.HIO_B_MLP', 'model.layers.1.mlp.HIO_A_down_proj', 'model.layers.1.mlp.HIO_B_down_proj', 'model.layers.1.mlp.HSM_mask_proxy', 'model.layers.10.HIO_A_Attn', 'model.layers.10.HIO_A_MLP', 'model.layers.10.HIO_B_Attn', 'model.layers.10.HIO_B_MLP', 'model.layers.10.mlp.HIO_A_down_proj', 'model.layers.10.mlp.HIO_B_down_proj', 'model.layers.10.mlp.HSM_mask_proxy', 'model.layers.11.HIO_A_Attn', 'model.layers.11.HIO_A_MLP', 'model.layers.11.HIO_B_Attn', 'model.layers.11.HIO_B_MLP', 'model.layers.11.mlp.HIO_A_down_proj', 'model.layers.11.mlp.HIO_B_down_proj', 'model.layers.11.mlp.HSM_mask_proxy', 'model.layers.12.HIO_A_Attn', 'model.layers.12.HIO_A_MLP', 'model.layers.12.HIO_B_Attn', 'model.layers.12.HIO_B_MLP', 'model.layers.12.mlp.HIO_A_down_proj', 'model.layers.12.mlp.HIO_B_down_proj', 'model.layers.12.mlp.HSM_mask_proxy', 'model.layers.13.HIO_A_Attn', 'model.layers.13.HIO_A_MLP', 'model.layers.13.HIO_B_Attn', 'model.layers.13.HIO_B_MLP', 'model.layers.13.mlp.HIO_A_down_proj', 'model.layers.13.mlp.HIO_B_down_proj', 'model.layers.13.mlp.HSM_mask_proxy', 'model.layers.14.HIO_A_Attn', 'model.layers.14.HIO_A_MLP', 'model.layers.14.HIO_B_Attn', 'model.layers.14.HIO_B_MLP', 'model.layers.14.mlp.HIO_A_down_proj', 'model.layers.14.mlp.HIO_B_down_proj', 'model.layers.14.mlp.HSM_mask_proxy', 'model.layers.15.HIO_A_Attn', 'model.layers.15.HIO_A_MLP', 'model.layers.15.HIO_B_Attn', 'model.layers.15.HIO_B_MLP', 'model.layers.15.mlp.HIO_A_down_proj', 'model.layers.15.mlp.HIO_B_down_proj', 'model.layers.15.mlp.HSM_mask_proxy', 'model.layers.16.HIO_A_Attn', 'model.layers.16.HIO_A_MLP', 'model.layers.16.HIO_B_Attn', 'model.layers.16.HIO_B_MLP', 'model.layers.16.mlp.HIO_A_down_proj', 'model.layers.16.mlp.HIO_B_down_proj', 'model.layers.16.mlp.HSM_mask_proxy', 'model.layers.17.HIO_A_Attn', 'model.layers.17.HIO_A_MLP', 'model.layers.17.HIO_B_Attn', 'model.layers.17.HIO_B_MLP', 'model.layers.17.mlp.HIO_A_down_proj', 'model.layers.17.mlp.HIO_B_down_proj', 'model.layers.17.mlp.HSM_mask_proxy', 'model.layers.18.HIO_A_Attn', 'model.layers.18.HIO_A_MLP', 'model.layers.18.HIO_B_Attn', 'model.layers.18.HIO_B_MLP', 'model.layers.18.mlp.HIO_A_down_proj', 'model.layers.18.mlp.HIO_B_down_proj', 'model.layers.18.mlp.HSM_mask_proxy', 'model.layers.19.HIO_A_Attn', 'model.layers.19.HIO_A_MLP', 'model.layers.19.HIO_B_Attn', 'model.layers.19.HIO_B_MLP', 'model.layers.19.mlp.HIO_A_down_proj', 'model.layers.19.mlp.HIO_B_down_proj', 'model.layers.19.mlp.HSM_mask_proxy', 'model.layers.2.HIO_A_Attn', 'model.layers.2.HIO_A_MLP', 'model.layers.2.HIO_B_Attn', 'model.layers.2.HIO_B_MLP', 'model.layers.2.mlp.HIO_A_down_proj', 'model.layers.2.mlp.HIO_B_down_proj', 'model.layers.2.mlp.HSM_mask_proxy', 'model.layers.20.HIO_A_Attn', 'model.layers.20.HIO_A_MLP', 'model.layers.20.HIO_B_Attn', 'model.layers.20.HIO_B_MLP', 'model.layers.20.mlp.HIO_A_down_proj', 'model.layers.20.mlp.HIO_B_down_proj', 'model.layers.20.mlp.HSM_mask_proxy', 'model.layers.21.HIO_A_Attn', 'model.layers.21.HIO_A_MLP', 'model.layers.21.HIO_B_Attn', 'model.layers.21.HIO_B_MLP', 'model.layers.21.mlp.HIO_A_down_proj', 'model.layers.21.mlp.HIO_B_down_proj', 'model.layers.21.mlp.HSM_mask_proxy', 'model.layers.22.HIO_A_Attn', 'model.layers.22.HIO_A_MLP', 'model.layers.22.HIO_B_Attn', 'model.layers.22.HIO_B_MLP', 'model.layers.22.mlp.HIO_A_down_proj', 'model.layers.22.mlp.HIO_B_down_proj', 'model.layers.22.mlp.HSM_mask_proxy', 'model.layers.23.HIO_A_Attn', 'model.layers.23.HIO_A_MLP', 'model.layers.23.HIO_B_Attn', 'model.layers.23.HIO_B_MLP', 'model.layers.23.mlp.HIO_A_down_proj', 'model.layers.23.mlp.HIO_B_down_proj', 'model.layers.23.mlp.HSM_mask_proxy', 'model.layers.24.HIO_A_Attn', 'model.layers.24.HIO_A_MLP', 'model.layers.24.HIO_B_Attn', 'model.layers.24.HIO_B_MLP', 'model.layers.24.mlp.HIO_A_down_proj', 'model.layers.24.mlp.HIO_B_down_proj', 'model.layers.24.mlp.HSM_mask_proxy', 'model.layers.25.HIO_A_Attn', 'model.layers.25.HIO_A_MLP', 'model.layers.25.HIO_B_Attn', 'model.layers.25.HIO_B_MLP', 'model.layers.25.mlp.HIO_A_down_proj', 'model.layers.25.mlp.HIO_B_down_proj', 'model.layers.25.mlp.HSM_mask_proxy', 'model.layers.26.HIO_A_Attn', 'model.layers.26.HIO_A_MLP', 'model.layers.26.HIO_B_Attn', 'model.layers.26.HIO_B_MLP', 'model.layers.26.mlp.HIO_A_down_proj', 'model.layers.26.mlp.HIO_B_down_proj', 'model.layers.26.mlp.HSM_mask_proxy', 'model.layers.27.HIO_A_Attn', 'model.layers.27.HIO_A_MLP', 'model.layers.27.HIO_B_Attn', 'model.layers.27.HIO_B_MLP', 'model.layers.27.mlp.HIO_A_down_proj', 'model.layers.27.mlp.HIO_B_down_proj', 'model.layers.27.mlp.HSM_mask_proxy', 'model.layers.3.HIO_A_Attn', 'model.layers.3.HIO_A_MLP', 'model.layers.3.HIO_B_Attn', 'model.layers.3.HIO_B_MLP', 'model.layers.3.mlp.HIO_A_down_proj', 'model.layers.3.mlp.HIO_B_down_proj', 'model.layers.3.mlp.HSM_mask_proxy', 'model.layers.4.HIO_A_Attn', 'model.layers.4.HIO_A_MLP', 'model.layers.4.HIO_B_Attn', 'model.layers.4.HIO_B_MLP', 'model.layers.4.mlp.HIO_A_down_proj', 'model.layers.4.mlp.HIO_B_down_proj', 'model.layers.4.mlp.HSM_mask_proxy', 'model.layers.5.HIO_A_Attn', 'model.layers.5.HIO_A_MLP', 'model.layers.5.HIO_B_Attn', 'model.layers.5.HIO_B_MLP', 'model.layers.5.mlp.HIO_A_down_proj', 'model.layers.5.mlp.HIO_B_down_proj', 'model.layers.5.mlp.HSM_mask_proxy', 'model.layers.6.HIO_A_Attn', 'model.layers.6.HIO_A_MLP', 'model.layers.6.HIO_B_Attn', 'model.layers.6.HIO_B_MLP', 'model.layers.6.mlp.HIO_A_down_proj', 'model.layers.6.mlp.HIO_B_down_proj', 'model.layers.6.mlp.HSM_mask_proxy', 'model.layers.7.HIO_A_Attn', 'model.layers.7.HIO_A_MLP', 'model.layers.7.HIO_B_Attn', 'model.layers.7.HIO_B_MLP', 'model.layers.7.mlp.HIO_A_down_proj', 'model.layers.7.mlp.HIO_B_down_proj', 'model.layers.7.mlp.HSM_mask_proxy', 'model.layers.8.HIO_A_Attn', 'model.layers.8.HIO_A_MLP', 'model.layers.8.HIO_B_Attn', 'model.layers.8.HIO_B_MLP', 'model.layers.8.mlp.HIO_A_down_proj', 'model.layers.8.mlp.HIO_B_down_proj', 'model.layers.8.mlp.HSM_mask_proxy', 'model.layers.9.HIO_A_Attn', 'model.layers.9.HIO_A_MLP', 'model.layers.9.HIO_B_Attn', 'model.layers.9.HIO_B_MLP', 'model.layers.9.mlp.HIO_A_down_proj', 'model.layers.9.mlp.HIO_B_down_proj', 'model.layers.9.mlp.HSM_mask_proxy']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/22 15:20:07 - OpenCompass - INFO - Try to load the data from /data/kris/shared_data/datasets/NLP/opencompass/./data/gsm8k/
gsm8k_6 train 7473
gsm8k_6 test 1319
08/22 15:20:07 - OpenCompass - INFO - Start inferencing [Llama-3.2-3B_hf/gsm8k_6]
[2025-08-22 15:20:07,710] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting build dataloader
[2025-08-22 15:20:07,710] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...
  0%|          | 0/11 [00:00<?, ?it/s]/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  9%|▉         | 1/11 [00:41<06:50, 41.00s/it]  9%|▉         | 1/11 [01:15<12:31, 75.13s/it]
Traceback (most recent call last):
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/opencompass/tasks/openicl_infer.py", line 161, in <module>
    inferencer.run()
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/opencompass/tasks/openicl_infer.py", line 89, in run
    self._inference()
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/opencompass/tasks/openicl_infer.py", line 134, in _inference
    inferencer.inference(retriever,
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/opencompass/openicl/icl_inferencer/icl_gen_inferencer.py", line 153, in inference
    results = self.model.generate_from_template(
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/opencompass/models/base.py", line 201, in generate_from_template
    return self.generate(inputs, max_out_len=max_out_len, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/opencompass/models/huggingface_above_v4_33.py", line 583, in generate
    outputs = self.model.generate(**tokens, **generation_kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/peft-0.15.1/src/peft/peft_model.py", line 1874, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/generation/utils.py", line 2463, in generate
    result = self._sample(
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/generation/utils.py", line 3432, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/models/llama/modeling_llama.py", line 1148, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/models/llama/modeling_llama.py", line 894, in forward
    layer_outputs = decoder_layer(
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/models/llama/modeling_llama.py", line 555, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/transformers-4.51.1/src/transformers/models/llama/modeling_llama.py", line 339, in forward
    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kris/miniconda3/envs/opencompass-pat/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kris/workspace/qianxuzhen/Pruning-LLMs/thirdparty/peft-0.15.1/src/peft/tuners/lora/layer.py", line 744, in forward
    result = result.to(torch_result_dtype)
KeyboardInterrupt
